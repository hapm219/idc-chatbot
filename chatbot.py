# chatbot.py

import os
import time
import textwrap
import torch
from langdetect import detect
from contextlib import contextmanager
from sentence_transformers import CrossEncoder
from llama_index.core import StorageContext, load_index_from_storage

from config import Config
from load_model import load_model_and_tokenizer, setup_llm, setup_embed_model
from clean_response import clean_response

HF_TOKEN = os.getenv("HF_TOKEN") or (lambda: (_ for _ in ()).throw(ValueError("âŒ ChÆ°a cÃ³ HF_TOKEN!")))()

@contextmanager
def timer():
    start = time.time()
    yield
    print(f"â±ï¸ Thá»i gian pháº£n há»“i: {time.time() - start:.2f} giÃ¢y")

def setup_index(storage_dir):
    storage_context = StorageContext.from_defaults(persist_dir=storage_dir)
    return load_index_from_storage(storage_context)

# Thiáº¿t láº­p
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âš™ï¸ Äang sá»­ dá»¥ng thiáº¿t bá»‹: {device.upper()} - {torch.cuda.get_device_name(0) if device == 'cuda' else 'CPU'}")

try:
    setup_embed_model()
    model, tokenizer = load_model_and_tokenizer(HF_TOKEN)
    llm = setup_llm(model, tokenizer)
    index = setup_index(Config.STORAGE_DIR)
    query_engine = index.as_query_engine(llm=llm, similarity_top_k=3)
    reranker = CrossEncoder("BAAI/bge-reranker-base")
except Exception as e:
    print(f"âŒ Lá»—i khá»Ÿi táº¡o: {e}")
    exit(1)

print("ğŸ¤– Chatbot IDC Ä‘Ã£ sáºµn sÃ ng. GÃµ 'exit' Ä‘á»ƒ thoÃ¡t.\n")

# VÃ²ng láº·p chÃ­nh
while True:
    user_input = input("ğŸ‘¤ Báº¡n: ")
    if user_input.lower() in ["exit", "quit"]:
        print("ğŸ‘‹ Táº¡m biá»‡t!")
        break

    lang = detect(user_input)
    print(f"ğŸŒ NgÃ´n ngá»¯ phÃ¡t hiá»‡n: {lang}")
    print("ğŸ¤” Chatbot Ä‘ang suy nghÄ©...")

    with timer():
        results = query_engine.retrieve(user_input)
        pairs = [(user_input, r.node.text) for r in results]
        scores = reranker.predict(pairs)
        top_node = results[scores.argmax()].node.text

        system_prompt = f"""
        Báº¡n lÃ  trá»£ lÃ½ AI tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u ná»™i bá»™. Tráº£ lá»i chÃ­nh xÃ¡c theo dá»¯ liá»‡u.
        Náº¿u khÃ´ng cháº¯c cháº¯n, hÃ£y nÃ³i rÃµ lÃ  chÆ°a tÃ¬m tháº¥y trong dá»¯ liá»‡u.
        CÃ¢u há»i: {user_input}
        """
        response = llm.complete(top_node + "\n" + system_prompt)
        cleaned_text = clean_response(response)
        wrapped_text = textwrap.fill(cleaned_text, width=100)

        print("ğŸ“„ Äoáº¡n vÄƒn mÃ´ hÃ¬nh Ä‘ang dÃ¹ng Ä‘á»ƒ tráº£ lá»i:")
        print("-", top_node[:300])
        prefix = "(Tiáº¿ng Viá»‡t)" if lang == "vi" else "(English)"
        print("ğŸ’¬", prefix, wrapped_text)